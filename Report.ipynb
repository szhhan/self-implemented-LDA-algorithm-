{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent Dirichlet Allocation (LDA) is a common method of topic modelling. It’s a technique that automatically discover the topics that documents contain. It’s a generative topic probabilistic first proposed by David M.Blei, Andrew Y.Ng, and Michae l. Jordan to find the latent variables(topics) in a text corpus (collection of Documents). \n",
    "\n",
    "Latent Dirichlet Allocation is a three level hierarchical Bayesian model for collections of discrete data and implemented by variantional EM algorithm. Variantional inference is a method that approximates probability densities through optimizations and it’s faster and easier to implement & scale than the traditional method such as MCMC. To LDA, a document is just a collection of topics where each topics has a particular probability of generating specific words. We implement variantional EM algorithm to calculate the optimal parameters of probabilistic topic distributions of documents and probabilistic words distributions over topics by using techniques such as posterior inference, Newton-Raphson method, gradient descent, Hessian matrix, etc. By getthing these optimal parameters, we can assign each documents to different topics by probabilities. \n",
    "\n",
    "In this report, we will first introduce and analyze the LDA model in a mathematical way. And then we will implement the core functions and present the pseudo-codes of the LDA model. We also find ways to optimize our algorithm and at last we will use the simulated data and real world data to test the accuracy of our model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backgrounds "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The research paper we are using is the paper Latent Dirichlet Allocation written by David M Blei, Andrew Y.Ng and Michael I.Jordan. LDA is a common method of topic modelling. A short example is if I have a document and I don't know it’s belonging to the food topic or the fruit topic. After implementing the LDA model I will know the percentage distribution of topics of that document. LDA is not only restricted to topic modelling in text, actually it can be work on any set that can be represented as binary vectors or non binary vectors. However, the most popular usage of LDA is still the topic modelling in text. In order to give a more clear picture on the content of the model, we will present a more specific example. \n",
    "\n",
    "Suppose we have the following set of the sentences (documents):\n",
    "\n",
    "1.\tThere are elephants, tigers, lions, dogs in the city’s zoo. \n",
    "2.\tEat an apple everyday, doctors go away. \n",
    "3.\tHunters said that there are tigers, bears and other wild animals in the forest so it’s very dangerous to go inside the forest alone. \n",
    "4.\tSusan loves to eat tomatoes, broccoli and apple. \n",
    "5.\tUnlike tigers who like to eat meats, rabbits love to eat carrot. \n",
    "\n",
    "LDA is a method that can discover the topics underlying each documents by self-learning:\n",
    "1 & 3: 100% topic A\n",
    "2 % 4: 100 topic B\n",
    "5: 75% topic A and 25% topic B. \n",
    "\n",
    "Topic A: 40% tigers, 20% elephants, …. \n",
    "Topic B: 30% apple, 20% broccoli,… \n",
    "\n",
    "We will introduce the mathematical inductions of LDA and how LDA performs the discovery in the next section. \n",
    "\n",
    "We need LDA because in the real world, the dataset can be very huge with a lot of documents and information. So we need the tools that can search, organize and understand that vast amount of information. LDA provides us a method to summarize that large collections of information and discover the hidden topics under each documents. This is the most known application of LDA in the real world: topic modellings. We can achieve some basic text minings by using the LDA such as novelty detection, classification, relevance judgments and similarity. The advantage of LDA is that it’s easy to implement, very clear logic and it’s a probabilistic model (soft-clustering) with interpretable topics. The limitations of the LDA is that the method assumes the number of topics in the dataset are specified by the user or based on a poisson distribution, but this is subjective and always did not actually highlight the true distribution of the topics. Second limitation is the relatively long running time than the other natural language processing alrogithms. The thrid limitation is that LDA did not consider the order of the words when it's learning. Sometimes the orders of the word may also affect the topic that document belonging to.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Declaration: \n",
    "\n",
    "K: number of topics \n",
    "\n",
    "M: number of documents\n",
    "\n",
    "N: Number of words in the documents \n",
    "\n",
    "$\\theta_m$: distribution for document m\n",
    "\n",
    "$\\alpha$: a K dimensional vector, the topic distribution of K topics. \n",
    "\n",
    "$\\beta$: K x V matrix, $\\beta_{ij}$ means the probability of $j^{th}$ word of $i^{th}$ topic. \n",
    "\n",
    "z: topics \n",
    "\n",
    "w: words \n",
    "\n",
    "D: Collection of Documents \n",
    "\n",
    "$\\phi$: M x N x K matrix, means probability distribution of each topic of words in each documents. \n",
    "\n",
    "$\\gamma$: M x K matrix, means the probability distribution of topics for each documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA assumes that the latent topics of a document comes from a multinomial distribution and the words also comes from a multinomial distribution conditional on the latent topics.\n",
    "\n",
    "In this problem,we have a set of M documents, D, which are mixture of latent topics we have a dictionary of unique words,W, with size V. We donate N as the number of words in a document.\n",
    "\n",
    "The generative process of LDA is described below.\n",
    "\n",
    "- 1. choose N (the number of words in the document) from a poisson distribution with $\\lambda=\\xi$, $N\\sim Poisson(\\xi)$\n",
    "    \n",
    "- 2. choose $\\theta\\sim Dir(\\alpha)$ \n",
    "\n",
    "- for each of N words $w_n$:\n",
    "        \n",
    "- (a) choose a topic $z_n \\sim Multinomial(\\theta)$\n",
    "\n",
    "- (b) choose a word $w_n \\sim Multinomial(\\beta_{z_n})$, $\\beta \\in \\mathbb{R}^{k \\times V}$ and $\\beta_{z_n}$ is the $z_n$ row of the $\\beta$\n",
    "\n",
    "The probability density of Dirichlet distribution is $p(\\theta|\\alpha)=\\frac{\\Gamma(\\sum_{i=1}^k\\alpha_i)}{\\prod_{i=1}^k\\Gamma(\\alpha_i)}\\theta_1^{\\alpha_1-1}...\\theta_k^{\\alpha_k-1}$ where $\\alpha \\in \\mathbb{R}^k$ ,$\\theta \\in \\mathbb{R}^k$ and $\\sum_{i=1}^k\\theta_i=1$, $\\alpha_i\\gt0$\n",
    "\n",
    "The Dirichlet distribution is an expansion of Gamma distribution into multivariable space and it is the conjugate distribution of multinomial distribution which can make our calculation much more easier.\n",
    "\n",
    "From the assumption above, we can easily derive following equations.\n",
    "\n",
    "$p(\\theta,z,w|\\alpha,\\beta)=p(\\theta|\\alpha)\\prod_{n=1}^Np(z_n|\\theta)p(w_n|z_n,\\beta)$\n",
    "\n",
    "\n",
    "$p(\\theta,w|\\alpha,\\beta) = p(\\theta|\\alpha)\\prod_{n=1}^N(\\sum_{z_n}p(z_n|\\theta)p(w_n|z_n,\\beta))$\n",
    "\n",
    "The likelihood of a single document can be expressed as\n",
    "\n",
    "$$p(w|\\alpha,\\beta)=\\int p(\\theta|\\alpha)\\prod_{n=1}^N(\\sum_{z_n}p(z_n|\\theta)p(w_n|z_n,\\beta))d\\theta$$\n",
    "\n",
    "And the likelihood of Corpus is:\n",
    "\n",
    "$$\n",
    "p(D|\\alpha,\\beta)=\\prod_{d=1}^M\\int p(\\theta_d|\\alpha)\\prod_{n=1}^{N_d}(\\sum_{z_n}p(z_n|\\theta_d)p(w_n|z_n,\\beta))d\\theta_d\n",
    "$$\n",
    "\n",
    "The problem is transformed to finding the \n",
    "\n",
    "$$\n",
    "\\alpha^*,\\beta^* = argmax_{\\alpha^*,\\beta^*}p(D|\\alpha,\\beta)\n",
    "$$\n",
    "\n",
    "Since there is no closed form solution, we can use the variant EM algrithm below to solve this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E step "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior distribution of hidden variables that we have to compute is:\n",
    "\n",
    "$$p(\\theta,\\textbf{z}|\\textbf{w},\\alpha,\\beta) = \\frac{p(\\theta,\\textbf{z},\\textbf{w}|\\alpha,\\beta)}{p(\\textbf{w}|\\alpha,\\beta)}$$\n",
    "\n",
    "Unfortunately, this distribution is intractable to compute in general. \n",
    "\n",
    "So indeed we have to use the variaitonal distribution on the latent variables which is:\n",
    "\n",
    "$q(\\theta,\\textbf{z}|\\gamma,\\phi) = q(\\theta|\\gamma)\\prod q(z_n|\\phi_n)$ where the Dirichlet parameter $\\gamma$ and the multinomial parameter $(\\phi_1,...,\\phi_n)$ are the free variational parameters. \n",
    "\n",
    "Then the next step will be set up an optimization problem that to determine the values of variational parameters $\\gamma$ and $\\phi$:\n",
    "\n",
    "$$(\\gamma^*,\\phi^*) = arg min_{(\\gamma,\\phi)}D(q(\\theta,\\textbf{z}|\\gamma,\\phi)||p(\\theta,\\textbf{z}|\\textbf{w},\\alpha,\\beta))$$\n",
    "\n",
    "Then we use the method of the KL divergence:\n",
    "\n",
    "the KL divergence between $q(\\theta,\\textbf{z}|\\gamma,\\phi)$ and $p(\\theta,\\textbf{z}|\\textbf{w},\\alpha,\\beta)$ is \n",
    "\n",
    "$$\\begin{split}\n",
    "&E[log q(\\theta,\\textbf{z}|\\gamma,\\phi)] - E[logp(\\theta,\\textbf{z}|\\textbf{w},\\alpha,\\beta)]\\\\\n",
    "&=E[log q(\\theta,\\textbf{z}|\\gamma,\\phi)] - E[logp(\\theta,\\textbf{z},\\textbf{w}|\\alpha,\\beta)] + log p(\\textbf{w}|\\alpha,\\beta)\n",
    "\\end{split}$$\n",
    "\n",
    "\n",
    "By applying the Jensen's equality:\n",
    "\n",
    "$$\\begin{split}\n",
    "log p(\\textbf{w}|\\alpha,\\beta) \n",
    "&= log \\int \\sum p(\\theta,\\textbf{z},\\textbf{w}|\\alpha,\\beta)d\\theta\\\\\n",
    "&=log \\int \\sum \\frac{p(\\theta,\\textbf{z},\\textbf{w}|\\alpha,\\beta)q(\\theta,\\textbf{z})}{q(\\theta,\\textbf{z})d\\theta}\\\\\n",
    "&\\geq \\int \\sum q(\\theta,\\textbf{z})log p(\\theta,\\textbf{z},\\textbf{w}|\\alpha,\\beta)d\\theta - \\int\\sum q(\\theta,\\textbf{z}) log q(\\theta,\\textbf{z})d\\theta\\\\\n",
    "&=E[log p(\\theta,\\textbf{z},\\textbf{w}|\\alpha,\\beta)] - E[q(\\theta,\\textbf{z})]\n",
    "\\end{split}$$\n",
    "\n",
    "\n",
    "Let $L(\\gamma,\\phi;\\alpha,\\beta) = E[logp(\\theta,\\textbf{z},\\textbf{w}|\\alpha,\\beta)] - E[log q(\\theta,\\textbf{z}]$ then we have:\n",
    "\n",
    "$$log p(\\textbf{w}|\\alpha,\\beta) = L(\\gamma,\\phi;\\alpha,\\beta) + D(q(\\theta,\\textbf{z}|\\gamma,\\phi)||p(\\theta,\\textbf{z}|\\textbf{w},\\alpha,\\beta))$$\n",
    "\n",
    "So maximizing the lower bound $L(\\gamma,\\phi;\\alpha,\\beta)$ is equivalent to minimizing the KL divergence between between the variational posterior probabilty and the true posterior probability. \n",
    "\n",
    "$$L(\\gamma,\\phi;\\alpha,\\beta) = E[log p(\\theta|\\alpha)] + E[log p(\\textbf{z}|\\theta)] + E[log p(\\textbf{w}|\\textbf{z},\\beta)] - E[log q(\\theta) - E[log q(z)]]$$\n",
    "\n",
    "We set the derivative of L and set them equal to 0, we obtain the updated equation of $\\phi$ and $\\gamma$:\n",
    "\n",
    "$$\\phi_{ni} \\propto \\beta_{iv}exp{\\psi(\\gamma_i)-\\psi(\\sum \\gamma_j)}$$\n",
    "\n",
    "$$\\gamma_i = \\alpha_i + \\sum \\phi_{ni}$$\n",
    "\n",
    "where $\\psi$ is the digamma function, which is the first derivative of the log gamma. \n",
    "\n",
    "\n",
    "So the pseudocode is:\n",
    "\n",
    "initialize $\\phi_{ni}^0 = \\frac{1}{K}$ for all i and n. \n",
    "\n",
    "initialize $\\gamma_i^0 = \\alpha_i + \\frac{N}{K}$ for all i. \n",
    "\n",
    "repeat:\n",
    "\n",
    "for n = 1 to N:\n",
    "\n",
    "for i = 1 to K:\n",
    "\n",
    "$\\phi_{ni}^{t+1} = \\beta_{iw_{n}}exp({\\psi(\\gamma_i^t)-\\psi(\\sum \\gamma_j^t)})$. (I think there's a typo in the origin paper of the pscedocode part. I think the correct updating way is like that.) \n",
    "\n",
    "normalize $\\phi_{ni}^{t+1}$ to make their sums to 1.\n",
    "\n",
    "$\\gamma^{t+1} = \\alpha + \\sum \\phi_{ni}^{t+1}$. \n",
    "\n",
    "until convergence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M step "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "M step is finding the MLE(maximum likelihood estimates) with expected sufficient statistics.\n",
    "\n",
    "\n",
    "Since the $\\alpha$, $\\beta$ is isolated with expectated sufficient statistics, we can isolate the log likelihood function with two term.\n",
    "\n",
    "\n",
    "First, we find the new $\\alpha$.\n",
    "\n",
    "$$L_{\\alpha} = \\sum_{d=1}^M(log(\\sum_{j=1}^k\\alpha_i)-\\sum_{i=1}^klog\\alpha_i+\\sum_{i=1}^k(\\alpha_i-1)(\\Phi(\\gamma_di)-\\Phi(\\sum_{j=1}^k\\gamma_dj))))$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial\\alpha_i}=M(\\Phi(\\sum_{j=1}^k\\alpha_j)-\\Phi(\\alpha_i))+\\sum_{d=1}^M(\\Phi(\\gamma_{di}-\\Phi(\\sum_{j=1}^k\\gamma_{dj})))$$\n",
    "\n",
    "The derivative depends on $\\alpha$, and therefore we need to derive a iterative way to find the optimal solution.\n",
    "\n",
    "Further we can get\n",
    "\n",
    "$\\frac{\\partial L}{\\partial\\alpha_i\\alpha_j} = M\\Phi'(\\sum_{j=1}^k\\alpha_j)-\\delta(i,j)M\\Phi'(\\alpha_i)$\n",
    "Newton-Raphson algorithm to find the optimal solution.\n",
    "\n",
    "Since the Hessian matrix can be written as $H = diag(-M\\Phi'(\\alpha))+1(M\\Phi'(\\sum_{j=1}^k\\alpha_j))1^T$\n",
    "\n",
    "To generilize it, $H=diag(h)+1z1^T$ and applying the matrix inverse lemma we can get $H^{-1}=dig(h)^{-1}-\\frac{diag(h)^{-1}11^Tdiag(h)^{-1}}{z^{-1}+\\sum_{j=1}^kh_j^{-1}}$ \n",
    "\n",
    "Therefore $(H^{-1}g)_i=\\frac{g_i-c}{h_i}$ where $c=\\frac{\\sum_{j=1}^kg_j/h_j}{z^{-1}+\\sum_{j=1}^kh_j^{-1}}$\n",
    "\n",
    "and we can use the update formula $\\alpha^{t+1} = \\alpha^{t}-H^{-1}g$ to get the result in a linear time complexity.\n",
    "\n",
    "\n",
    "Then, we find the new $\\beta$\n",
    "\n",
    "Since the $\\beta$ is constrained that $\\sum_{j=1}^V\\beta_ij=1$ for any $i$\n",
    "\n",
    "By adding Lagrange Multipliers, we get\n",
    "\n",
    "$L_{[\\beta]}=\\sum_{d=1}^M\\sum_{n=1}^{N_d}\\sum_{i=1}^k\\sum_{j=1}^V \\psi_{dni}W^j_{dn}log\\beta_{ij}+\\sum_{i=1}^k\\lambda_i(\\sum_{j=1}^V\\beta_{ij}-1)$\n",
    "\n",
    "Taking the derivative with repect to $\\beta_{ij}$, we get\n",
    "\n",
    "$$\\frac{\\partial L_{[\\beta]}}{\\partial\\beta_{ij}} =\\sum_{d=1}^M\\sum_{n=1}^{N_d} \\psi_{dni}W^j_{dn}\\frac{1}{\\beta_{ij}}+\\lambda_i$$\n",
    "\n",
    "Setting it to zeros,\n",
    "$$\\beta_{ij}=-\\frac{1}{\\lambda_i}\\sum_{d=1}^M\\sum_{n=1}^{N_d} \\psi_{dni}W^j_{dn}$$\n",
    "\n",
    "Therefore,\n",
    "$$\\beta_{ij} = \\frac{\\sum_{d=1}^M\\sum_{n=1}^{N_d} \\psi_{dni}W^j_{dn}}{\\sum_{d=1}^M\\sum_{n=1}^{N_d}\\sum_{j=1}^V \\psi_{dni}W^j_{dn}}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### M step pseudocode\n",
    "\n",
    "1. updating $\\alpha$\n",
    "\n",
    "using linear time Newton-Raphson algorithm to find optimal $\\alpha$\n",
    "\n",
    "repeat:\n",
    "\n",
    "for i=1 to k\n",
    "\n",
    "$g_i = M(\\Phi(\\sum_{j=1}^k\\alpha_j)-\\Phi(\\alpha_i))+\\sum_{d=1}^M(\\Phi(\\gamma_{di}-\\Phi(\\sum_{j=1}^k\\gamma_{dj})))$\n",
    "\n",
    "$h_i = -M\\Phi'(\\alpha_i)$\n",
    "\n",
    "$c = \\frac{\\sum_{j=1}^kg_j/h_j}{z^{-1}+\\sum_{j=1}^kh_j^{-1}}$\n",
    "\n",
    "$\\alpha_i^{t+1}=\\alpha_i^{t}-\\frac{g_i-c}{h_i}$\n",
    "\n",
    "until converge\n",
    "\n",
    "2. updating $\\beta$\n",
    "\n",
    "for i=1 to k:\n",
    "\n",
    "for j=1 to V:\n",
    "$$\\beta_{ij} = \\frac{\\sum_{d=1}^M\\sum_{n=1}^{N_d} \\psi_{dni}W^j_{dn}}{\\sum_{d=1}^M\\sum_{n=1}^{N_d}\\sum_{j=1}^V \\psi_{dni}W^j_{dn}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combinations \n",
    "\n",
    "So the procedure of our algorithm is basically:\n",
    "\n",
    "1. Initialize parameters: initiate number of topics and number of vocabs. Initiate alpha from gamma distribution and beta from the dirichlet distribution. \n",
    "\n",
    "2. E step: For each document, find the optimizing values of the variational parameter phi and gamma. \n",
    "\n",
    "3. M step: Maximizing the resulting lower bound on the log likelihood with respect to the model parameters alpha and beta. Find the converged alpha and beta. \n",
    "\n",
    "4. when all parameters converged, break \n",
    "\n",
    "5. output the parameter alpha, beta, phi and gamma. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use two methods to do the optimizations, and the result is very substantial: Within the same output, the running speed increases by more than 500%. \n",
    "\n",
    "The methods we use for optimizations are:\n",
    "\n",
    "1. Include another version of digamma function (referenced to https://people.sc.fsu.edu/~jburkardt/py_src/asa103/digamma.py). Because before the optimization, we use the digamma function from the library scipy. However, the digamma function in scipy costs a relatively long time to run and not so efficient. Therefore, we find another digamma version online which yields the same result but much more efficient than the digamma function in scipy. Because in both E steps and M steps we use a lot of calculation of digamma function. So improving the speed of the digamma function can largely improve the speed of the algorithm. \n",
    "\n",
    "2. Use Jit and Jit(nopython=True) to optimize the performance. Because some steps and functions contain library/function that are not supported by Jit(nopython=True), So I only use Jit to optimize (cannot improve a lot). But for the main part of E step and M step, I use Jit(nopython=True) because they only contain basic operations and some of the numpy. This can largely improve the performance of the speed. \n",
    "\n",
    "Some examples of code modifications: (E step)\n",
    "\n",
    "### Original version:\n",
    "\n",
    "def E_one_step(doc, V, alpha, beta, phi0, gamma0, tol=1e-3):\n",
    "\n",
    ".......\n",
    "\n",
    "for i in range(MAX_E_ITER):\n",
    "\n",
    "        for n in range(N):\n",
    "        \n",
    "            for j in range(topic_num):\n",
    "            \n",
    "                phi[n, j] = (beta[j,].T@doc[n,]) * np.exp(digamma(gamma[i])-digamma(sum(gamma)))\n",
    "                \n",
    "            phi[n,] = phi[n,] / np.sum(phi[n,])\n",
    "            \n",
    "For here digamma is the digamma in scipy. \n",
    "\n",
    "### Optimazation version: \n",
    "\n",
    "@jit(nopython=True)\n",
    "\n",
    "def E_one_step_numba(doc, V, alpha, beta, phi0, gamma0, tol=1e-3):\n",
    "\n",
    ".......\n",
    "\n",
    "for i in range(MAX_E_ITER):\n",
    "\n",
    "        for n in range(N):\n",
    "        \n",
    "            for j in range(topic_num):\n",
    "            \n",
    "                phi[n, j] = (beta[j,].T@doc[n,]) * np.exp(digamma2(gamma[i])-digamma2(np.sum(gamma)))\n",
    "                \n",
    "            phi[n,] = phi[n,] / np.sum(phi[n,])\n",
    "            \n",
    "        gamma = alpha + np.sum(phi,axis=0).T\n",
    "\n",
    "For here digamma2 is the new digamma imported from the online sources. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization results on the simulated dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ldaa import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs,a,b = data_simulation_numba(10,10,50)\n",
    "model = LDA(10,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.07 s ± 555 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit phi_post,gamma_post,alpha_post,beta_post = model.fit_numba(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.3 s ± 3.75 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit phi_post1,gamma_post1,alpha_post1,beta_post1 = model.fit(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that by using the timeit, the running time of the optimization version really improves a lot, from around 30 seconds to only 6 seconds. A very large improvement in running time and we will see that if the output varies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_post,gamma_post,alpha_post,beta_post = model.fit_numba(docs)\n",
    "phi_post1,gamma_post1,alpha_post1,beta_post1 = model.fit(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p1 = np.repeat(beta_post, 10, axis=0)\n",
    "p2 = np.repeat(beta_post1, 10, axis=0)\n",
    "p = [p1,p2] \n",
    "fig, axes = plt.subplots(1, 2, figsize=(12,12), sharey=True)\n",
    "for i, ax in enumerate(axes.ravel(), 1):\n",
    "    ax.imshow(p[i-1],cmap='Blues')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above two plots about the posterior Beta is identical. Therefore, we can conclude that with the same output, our optimization is very successful by improving the running speed by nearly 500%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part, we randomly generate docs according to the generative model for given $\\alpha$ and $\\beta$. However, measuring the MSE of estimated $\\beta$ and true $\\beta$ is not approperiate, the topics are not the in the same order and determining the coresponding topic is also hard. Therefore, we create $\\beta$ in following way. The intuition is that we want topics to be significantly different from each other. In real world, topics also have little overlap with each other.In addition, we make document to contain mostly three or two topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot above, the left is the true $\\beta$ we generated and the right is the estimated $\\beta$ from the model. Although there is some differences, the LDA did find most of the feature from the data and we consider the algorithm to be successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real World Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part, we use the real world document from the website: http://www.cs.columbia.edu/~blei/lda-c/ to do the testing. The txt file is saved in the github called ap.txt and the ipynb is real world data.ipynb. After processing of the txt file, we run our model to estimate alpha,beta, gamma and phi for the corpus. \n",
    "\n",
    "We only choose the first 500 documents to do the analysis because too many documents may cause a very long time to run. However, we still get a good result by discovering the beta parameter, which is the corresponding words in each topic. Mentioned, we select k=10 which means 10 topics to do the clusterings. \n",
    "\n",
    "If you are interested in which documents belongs to which topic or which words belongs to which topic, you can look at the gamma and phi we output. For here, we just list some of the beta value because of the limiting spaces, just list some of the high probability vocabularys in each of the topic:\n",
    "\n",
    "1. Government topic: government, federal, hospital, nations, american, party, service, agents, arrested, report, president, police, agreeement, california, countries, union, operations\n",
    "\n",
    "2. Financial Topic: Billion, chief, financial, soviet, authorities, economy, congress, percent, cents, meeting, trade, union, state\n",
    "\n",
    "3. International Economic Topic: months, people, spokesman, national, economic, party, market, york, prices, germany, official, news, bank, war, military\n",
    "\n",
    "4. International Political Topic: company, country, chief, leader, official, city, soviet, united, committee, president, bush, oil, political, east\n",
    "\n",
    "5. Others Topic: department, visit, plan, air, police, states, war, force, john, west, group\n",
    "\n",
    "6. Military Topic: country, government, soviet, largest, fire, congress, israel, military, state, time, trade, oil, support, officials\n",
    "\n",
    "7. People Topic: country, foreign, people, american, economic, party, public, house, minister, southern, police, administration, bill, trade\n",
    "\n",
    "8. Daily news Topic: saturday, friday, wednesday, thursday, year, people, business, condition, tuesday, day, working, washionton, week\n",
    "\n",
    "9. Entertainment Topic: people, rose, center, national, car, bush, than, state, billion, news, chairman \n",
    "\n",
    "10. Crime Topic: defense, national, american, soviet, party, house ,united, president, fell, washionton, church, shot, women, children, court \n",
    "\n",
    "\n",
    "Note that there are some problems when we are dealing with the stopwords, so there are some stopwords still appear in the documents. But in overall it does not affect the final result a lot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to other algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we compare the variational EM algorithm with variantional Bayes algorithm applied in the sklearn.We use the same $\\beta$ as we mentioned in the simulation part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Algorithm|variational EM|variational Bayes|\n",
    "|-|-|-|\n",
    "|time|32s|8.92s|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot above, the left is the true $\\beta$ we generated, the middle is the esitmated $\\beta$ from variational EM algorithm and the right is the estimated $\\beta$ from the variational Bayes. The variational Bayes has a better performance in time and accuracy since the variantional Bayes is proposed in 2010 which is 7 years late than variantional EM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
