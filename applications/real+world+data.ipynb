{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ldaa import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import digamma, polygamma\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('ap.txt', 'r')\n",
    "text = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['i',\"a\",\"about\",\"above\",\"after\",\"again\",\"against\",\"ain\",\"all\",\"am\",\"an\",\"and\",\"any\",\"are\",\n",
    "             \"aren\",\"aren't\",\"as\",\"at\",\"be\",\"because\",\"been\",\"before\",\"being\",\"below\",\"between\",\"both\",\"but\",\"by\",\n",
    "             \"can\",\"couldn\",\"couldn't\",\"d\",\"did\",\"didn\",\"didn't\",\"do\",\"does\",\"doesn\",\"doesn't\",\"doing\",\"don\",\"don't\",\n",
    "             \"down\",\"during\",\"each\",\"few\",\"for\",\"from\",\"further\",\"had\",\"hadn\",\"hadn't\",\"has\",\"hasn\",\"hasn't\",\"have\",\n",
    "             \"haven\",\"haven't\",\"having\",\"he\",\"her\",\"here\",\"hers\",\"herself\",\"him\",\"himself\",\"his\",\"how\",\"i\",\"if\",\"in\",\n",
    "             \"into\",\"is\",\"isn\",\"isn't\",\"it\",\"it's\",\"its\",\"itself\",\"just\",\"ll\",\"m\",\"ma\",\"me\",\"mightn\",\"mightn't\",\"more\",\n",
    "             \"most\",\"mustn\",\"mustn't\",\"my\",\"myself\",\"needn\",\"needn't\",\"no\",\"nor\",\"not\",\"now\",\"o\",\"of\",\"off\",\"on\",\n",
    "             \"once\",\"only\",\"or\",\"other\",\"our\",\"ours\",\"ourselves\",\"out\",\"over\",\"own\",\"re\",\"s\",\"same\",\"shan\",\"shan't\",\n",
    "             \"she\",\"she's\",\"should\",\"should've\",\"shouldn\",\"shouldn't\",\"so\",\"some\",\"such\",\"t\",\"than\",\"that\",\"that'll\",\n",
    "             \"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"there\",\"these\",\"they\",\"this\",\"those\",\"through\",\"to\",\n",
    "             \"too\",\"under\",\"until\",\"up\",\"ve\",\"very\",\"was\",\"wasn\",\"wasn't\",\"we\",\"were\",\"weren\",\"weren't\",\"what\",\"when\",\n",
    "             \"where\",\"which\",\"while\",\"who\",\"whom\",\"why\",\"will\",\"with\",\"won\",\"won't\",\"wouldn\",\"wouldn't\",\"y\",\"you\",\n",
    "             \"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"could\",\"he'd\",\"he'll\",\"he's\",\n",
    "             \"here's\",\"how's\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"let's\",\"ought\",\"she'd\",\"she'll\",\"that's\",\"there's\",\"they'd\",\n",
    "             \"they'll\",\"they're\",\"they've\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"what's\",\"when's\",\"where's\",\"who's\",\"why's\",\n",
    "             \"would\",\"able\",\"abst\",\"accordance\",\"according\",\"accordingly\",\"across\",\"act\",\"actually\",\"added\",\"adj\",\"affected\",\n",
    "             \"affecting\",\"affects\",\"afterwards\",\"ah\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"among\",\n",
    "             \"amongst\",\"announce\",\"another\",\"anybody\",\"anyhow\",\"anymore\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\n",
    "             \"apparently\",\"approximately\",\"arent\",\"arise\",\"around\",\"aside\",\"ask\",\"asking\",\"auth\",\"available\",\"away\",\"awfully\",\n",
    "             \"b\",\"back\",\"became\",\"become\",\"becomes\",\"becoming\",\"beforehand\",\"begin\",\"beginning\",\"beginnings\",\"begins\",\"behind\",\n",
    "             \"believe\",\"beside\",\"besides\",\"beyond\",\"biol\",\"brief\",\"briefly\",\"c\",\"ca\",\"came\",\"cannot\",\"can't\",\"cause\",\"causes\",\n",
    "             \"certain\",\"certainly\",\"co\",\"com\",\"come\",\"comes\",\"contain\",\"containing\",\"contains\",\"couldnt\",\"date\",\"different\",\n",
    "             \"done\",\"downwards\",\"due\",\"e\",\"ed\",\"edu\",\"effect\",\"eg\",\"eight\",\"eighty\",\"either\",\"else\",\"elsewhere\",\"end\",\"ending\",\n",
    "             \"enough\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\n",
    "             \"except\",\"f\",\"far\",\"ff\",\"fifth\",\"first\",\"five\",\"fix\",\"followed\",\"following\",\"follows\",\"former\",\"formerly\",\"forth\",\n",
    "             \"found\",\"four\",\"furthermore\",\"g\",\"gave\",\"get\",\"gets\",\"getting\",\"give\",\"given\",\"gives\",\"giving\",\"go\",\"goes\",\"gone\",\n",
    "             \"got\",\"gotten\",\"h\",\"happens\",\"hardly\",\"hed\",\"hence\",\"hereafter\",\"hereby\",\"herein\",\"heres\",\"hereupon\",\"hes\",\"hi\",\n",
    "             \"hid\",\"hither\",\"home\",\"howbeit\",\"however\",\"hundred\",\"id\",\"ie\",\"im\",\"immediate\",\"immediately\",\"importance\",\n",
    "             \"important\",\"inc\",\"indeed\",\"index\",\"information\",\"instead\",\"invention\",\"inward\",\"itd\",\"it'll\",\"j\",\"k\",\"keep\",\n",
    "             \"keeps\",\"kept\",\"kg\",\"km\",\"know\",\"known\",\"knows\",\"l\",\"largely\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\n",
    "             \"less\",\"lest\",\"let\",\"lets\",\"like\",\"liked\",\"likely\",\"line\",\"little\",\"'ll\",\"look\",\"looking\",\"looks\",\"ltd\",\"made\",\n",
    "             \"mainly\",\"make\",\"makes\",\"many\",\"may\",\"maybe\",\"mean\",\"means\",\"meantime\",\"meanwhile\",\"merely\",\"mg\",\"might\",\"million\",\n",
    "             \"miss\",\"ml\",\"moreover\",\"mostly\",\"mr\",\"mrs\",\"much\",\"mug\",\"must\",\"n\",\"na\",\"name\",\"namely\",\"nay\",\"nd\",\"near\",\"nearly\",\"necessarily\",\n",
    "             \"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"ninety\",\"nobody\",\"non\",\"none\",\"nonetheless\",\n",
    "             \"noone\",\"normally\",\"nos\",\"noted\",\"nothing\",\"nowhere\",\"obtain\",\"obtained\",\"obviously\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"omitted\",\n",
    "             \"one\",\"ones\",\"onto\",\"ord\",\"others\",\"otherwise\",\"outside\",\"overall\",\"owing\",\"p\",\"page\",\"pages\",\"part\",\"particular\",\"particularly\",\n",
    "             \"past\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"poorly\",\"possible\",\"possibly\",\"potentially\",\"pp\",\"predominantly\",\"present\",\"previously\",\n",
    "             \"primarily\",\"probably\",\"promptly\",\"proud\",\"provides\",\"put\",\"q\",\"que\",\"quickly\",\"quite\",\"qv\",\"r\",\"ran\",\"rather\",\"rd\",\"readily\",\"really\",\"recent\",\n",
    "             \"recently\",\"ref\",\"refs\",\"regarding\",\"regardless\",\"regards\",\"related\",\"relatively\",\"research\",\"respectively\",\"resulted\",\"resulting\",\"results\",\"right\",\n",
    "             \"run\",\"said\",\"saw\",\"say\",\"saying\",\"says\",\"sec\",\"section\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sent\",\"seven\",\n",
    "             \"several\",\"shall\",\"shed\",\"shes\",\"show\",\"showed\",\"shown\",\"showns\",\"shows\",\"significant\",\"significantly\",\"similar\",\"similarly\",\"since\",\"six\",\"slightly\",\n",
    "             \"somebody\",\"somehow\",\"someone\",\"somethan\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specifically\",\"specified\",\"specify\",\n",
    "             \"specifying\",\"still\",\"stop\",\"strongly\",\"sub\",\"substantially\",\"successfully\",\"sufficiently\",\"suggest\",\"sup\",\"sure\",\"take\",\"taken\",\"taking\",\"tell\",\"tends\",\n",
    "             \"th\",\"thank\",\"thanks\",\"thanx\",\"thats\",\"that've\",\"thence\",\"thereafter\",\"thereby\",\"thered\",\"therefore\",\"therein\",\"there'll\",\"thereof\",\"therere\",\"theres\",\n",
    "             \"thereto\",\"thereupon\",\"there've\",\"theyd\",\"theyre\",\"think\",\"thou\",\"though\",\"thoughh\",\"thousand\",\"throug\",\"throughout\",\"thru\",\"thus\",\"til\",\"tip\",\"together\",\n",
    "             \"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"ts\",\"twice\",\"two\",\"u\",\"un\",\"unfortunately\",\"unless\",\"unlike\",\"unlikely\",\"unto\",\"upon\",\n",
    "             \"ups\",\"us\",\"use\",\"used\",\"useful\",\"usefully\",\"usefulness\",\"uses\",\"using\",\"usually\",\"v\",\"value\",\"various\",\"'ve\",\"via\",\"viz\",\"vol\",\"vols\",\"vs\",\"w\",\"want\",\n",
    "             \"wants\",\"wasnt\",\"way\",\"wed\",\"welcome\",\"went\",\"werent\",\"whatever\",\"what'll\",\"whats\",\"whence\",\"whenever\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\n",
    "             \"wheres\",\"whereupon\",\"wherever\",\"whether\",\"whim\",\"whither\",\"whod\",\"whoever\",\"whole\",\"who'll\",\"whomever\",\"whos\",\"whose\",\"widely\",\"willing\",\"wish\",\n",
    "             \"within\",\"without\",\"wont\",\"words\",\"world\",\"wouldnt\",\"www\",\"x\",\"yes\",\"yet\",\"youd\",\"youre\",\"z\",\"zero\",\"a's\",\"ain't\",\"allow\",\"allows\",\"apart\",\"appear\",\n",
    "             \"appreciate\",\"appropriate\",\"associated\",\"best\",\"better\",\"c'mon\",\"c's\",\"cant\",\"changes\",\"clearly\",\"concerning\",\"consequently\",\"consider\",\"considering\",\n",
    "             \"corresponding\",\"course\",\"currently\",\"definitely\",\"described\",\"despite\",\"entirely\",\"exactly\",\"example\",\"going\",\"greetings\",\"hello\",\"help\",\"hopefully\",\n",
    "             \"ignored\",\"inasmuch\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"it'd\",\"keep\",\"keeps\",\"novel\",\"presumably\",\"reasonably\",\"second\",\"secondly\",\n",
    "             \"sensible\",\"serious\",\"seriously\",\"sure\",\"t's\",\"third\",\"thorough\",\"thoroughly\",\"three\",\"well\",\"wonder\",\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \n",
    "             \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \n",
    "             \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\",\n",
    "             \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\",\n",
    "             \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\",\n",
    "             \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \n",
    "             \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\",\n",
    "             \"don\", \"should\", \"now\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in text:\n",
    "    if '<DOC>\\n' in t or '</DOC' in t or 'TEXT>' in t or '<DOC' in t or '<DOCNO>' in t:\n",
    "        text.remove(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = list(map(lambda x: x.lower(),text))\n",
    "txt = list(map(lambda x: x.strip(),txt))\n",
    "txt = list(map(lambda x: re.sub('[0-9]+', '', x),txt))\n",
    "txt = list(map(lambda x: x.translate(str.maketrans('', '', string.punctuation)),txt))\n",
    "txt = list(map(lambda x: x.split(),txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tt in txt:\n",
    "    for i in tt:\n",
    "        if i in set(stopwords):\n",
    "            tt.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6750"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = txt[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "countt = {id: Counter(doc) for id, doc in enumerate(docs)}\n",
    "df = pd.DataFrame(countt).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = df.index\n",
    "ds = df.values.T.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['a', 'aahed', 'abandon', 'abandoned', 'abc', 'abcs', 'abdomen',\n",
       "       'aberdeen', 'abilities', 'ability',\n",
       "       ...\n",
       "       'zero', 'zircon', 'zirconium', 'zone', 'zones', 'zoo', 'zookeepers',\n",
       "       'zorenstein', 'zucaro', 'zwick'],\n",
       "      dtype='object', length=9571)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = {}\n",
    "for i,j in enumerate(words):\n",
    "    word_list[j] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "docss = []\n",
    "V = len(words)\n",
    "for k in range(500):\n",
    "    N = len(docs[k])\n",
    "    doc = np.zeros((N))\n",
    "    for i in range(N):\n",
    "        doc[i] = word_list[docs[k][i]] \n",
    "    docss.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_docs = []\n",
    "for k in range(500):\n",
    "    N = len(docs[k])\n",
    "    doc = np.zeros((N,V))\n",
    "    for i in range(N):\n",
    "        doc[i][int(docss[k][i])] = 1\n",
    "    final_docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86.694"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average([doc.shape[0] for doc in final_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LDA(10,V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0\n",
      "finished E\n",
      "finished M\n",
      "step 1\n",
      "finished E\n",
      "finished M\n",
      "step 2\n",
      "finished E\n",
      "finished M\n",
      "step 3\n",
      "finished E\n",
      "finished M\n",
      "step 4\n",
      "finished E\n",
      "finished M\n",
      "step 5\n",
      "finished E\n",
      "finished M\n",
      "step 6\n",
      "finished E\n",
      "finished M\n",
      "step 7\n",
      "finished E\n",
      "finished M\n",
      "step 8\n",
      "finished E\n",
      "finished M\n",
      "step 9\n",
      "finished E\n",
      "finished M\n",
      "step 10\n",
      "finished E\n",
      "finished M\n",
      "step 11\n",
      "finished E\n",
      "finished M\n",
      "step 12\n",
      "finished E\n",
      "finished M\n",
      "step 13\n",
      "finished E\n",
      "finished M\n",
      "step 14\n",
      "finished E\n",
      "finished M\n",
      "step 15\n",
      "finished E\n",
      "finished M\n",
      "step 16\n",
      "finished E\n",
      "finished M\n",
      "step 17\n",
      "finished E\n",
      "finished M\n",
      "step 18\n",
      "finished E\n",
      "finished M\n",
      "step 19\n",
      "finished E\n",
      "finished M\n",
      "step 20\n",
      "finished E\n",
      "finished M\n",
      "step 21\n",
      "finished E\n",
      "finished M\n",
      "step 22\n",
      "finished E\n",
      "finished M\n",
      "step 23\n",
      "finished E\n",
      "finished M\n",
      "step 24\n",
      "finished E\n",
      "finished M\n",
      "step 25\n",
      "finished E\n",
      "finished M\n",
      "step 26\n",
      "finished E\n",
      "finished M\n",
      "step 27\n",
      "finished E\n",
      "finished M\n",
      "step 28\n",
      "finished E\n",
      "finished M\n",
      "step 29\n",
      "finished E\n",
      "finished M\n",
      "step 30\n",
      "finished E\n",
      "finished M\n",
      "step 31\n",
      "finished E\n",
      "finished M\n",
      "step 32\n",
      "finished E\n",
      "finished M\n",
      "step 33\n",
      "finished E\n",
      "finished M\n",
      "step 34\n",
      "finished E\n",
      "finished M\n",
      "step 35\n",
      "finished E\n",
      "finished M\n",
      "step 36\n",
      "finished E\n",
      "finished M\n",
      "step 37\n",
      "finished E\n",
      "finished M\n",
      "step 38\n",
      "finished E\n",
      "finished M\n",
      "step 39\n",
      "finished E\n",
      "finished M\n",
      "step 40\n",
      "finished E\n",
      "finished M\n",
      "step 41\n",
      "finished E\n",
      "finished M\n",
      "step 42\n",
      "finished E\n",
      "finished M\n",
      "step 43\n",
      "finished E\n",
      "finished M\n",
      "step 44\n",
      "finished E\n",
      "finished M\n",
      "step 45\n",
      "finished E\n",
      "finished M\n",
      "step 46\n",
      "finished E\n",
      "finished M\n",
      "step 47\n",
      "finished E\n",
      "finished M\n",
      "step 48\n",
      "finished E\n",
      "finished M\n",
      "step 49\n",
      "finished E\n",
      "finished M\n",
      "step 50\n",
      "finished E\n",
      "finished M\n",
      "step 51\n",
      "finished E\n",
      "finished M\n",
      "step 52\n",
      "finished E\n",
      "finished M\n",
      "step 53\n",
      "finished E\n",
      "finished M\n",
      "step 54\n",
      "finished E\n",
      "finished M\n",
      "step 55\n",
      "finished E\n",
      "finished M\n",
      "step 56\n",
      "finished E\n",
      "finished M\n",
      "step 57\n",
      "finished E\n",
      "finished M\n",
      "step 58\n",
      "finished E\n",
      "finished M\n",
      "step 59\n",
      "finished E\n",
      "finished M\n",
      "step 60\n",
      "finished E\n",
      "finished M\n",
      "step 61\n",
      "finished E\n",
      "finished M\n",
      "step 62\n",
      "finished E\n",
      "finished M\n",
      "step 63\n",
      "finished E\n",
      "finished M\n",
      "step 64\n",
      "finished E\n",
      "finished M\n",
      "step 65\n",
      "finished E\n",
      "finished M\n",
      "step 66\n",
      "finished E\n",
      "finished M\n",
      "step 67\n",
      "finished E\n",
      "finished M\n",
      "step 68\n",
      "finished E\n",
      "finished M\n",
      "step 69\n",
      "finished E\n",
      "finished M\n",
      "step 70\n",
      "finished E\n",
      "finished M\n",
      "step 71\n",
      "finished E\n",
      "finished M\n",
      "step 72\n",
      "finished E\n",
      "finished M\n",
      "step 73\n",
      "finished E\n",
      "finished M\n",
      "step 74\n",
      "finished E\n",
      "finished M\n",
      "step 75\n",
      "finished E\n",
      "finished M\n",
      "step 76\n",
      "finished E\n",
      "finished M\n",
      "step 77\n",
      "finished E\n",
      "finished M\n",
      "step 78\n",
      "finished E\n",
      "finished M\n",
      "step 79\n",
      "finished E\n",
      "finished M\n",
      "step 80\n",
      "finished E\n",
      "finished M\n",
      "step 81\n",
      "finished E\n",
      "finished M\n",
      "step 82\n",
      "finished E\n",
      "finished M\n",
      "step 83\n",
      "finished E\n",
      "finished M\n",
      "step 84\n",
      "finished E\n",
      "finished M\n",
      "step 85\n",
      "finished E\n",
      "finished M\n",
      "step 86\n",
      "finished E\n",
      "finished M\n",
      "step 87\n",
      "finished E\n",
      "finished M\n",
      "step 88\n",
      "finished E\n",
      "finished M\n",
      "step 89\n",
      "finished E\n",
      "finished M\n",
      "step 90\n",
      "finished E\n",
      "finished M\n",
      "step 91\n",
      "finished E\n",
      "finished M\n",
      "step 92\n",
      "finished E\n",
      "finished M\n",
      "step 93\n",
      "finished E\n",
      "finished M\n",
      "step 94\n",
      "finished E\n",
      "finished M\n",
      "step 95\n",
      "finished E\n",
      "finished M\n",
      "step 96\n",
      "finished E\n",
      "finished M\n",
      "step 97\n",
      "finished E\n",
      "finished M\n",
      "step 98\n",
      "finished E\n",
      "finished M\n",
      "step 99\n",
      "finished E\n",
      "finished M\n"
     ]
    }
   ],
   "source": [
    "phi_post,gamma_post,alpha_post,beta_post = model.fit(final_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('phi.npy', phi_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('gamma.npy', gamma_post)\n",
    "np.save('alpha.npy', alpha_post)\n",
    "np.save('beta.npy', beta_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_f = np.load('beta.npy')\n",
    "alpha_f = np.load('alpha.npy')\n",
    "phi_f = np.load('phi.npy')\n",
    "gamma_f = np.load('gamma.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index(x0):\n",
    "    x = x0.tolist()\n",
    "    max_values = heapq.nlargest(100, x)\n",
    "    index = [0] * 100\n",
    "    for i in range(100):\n",
    "        index[i] = x.index(max_values[i])\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['government', 'and', 'hospital', 'year', 'people', 'federal', 'nations', 'american', 'had', 'party', 'service', 'agents', 'arrested', 'report', 'president', 'what', 'police', 'do', 'said', 'new', 'docno', 'dont', 'agreement', 'times', 'night', 'tuesday', 'california', 'ago', 'countries', 'call', 'operating', 'system', 'union']\n"
     ]
    }
   ],
   "source": [
    "word0 = []\n",
    "for key, value in word_list.items():\n",
    "    if value in find_index(beta_f[0]):\n",
    "        word0.append(key)\n",
    "for w in word0:\n",
    "    if w in stopwords:\n",
    "        word0.remove(w)\n",
    "print(word0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['billion', 'chief', 'area', 'friday', 'financial', 'general', 'soviet', 'monday', 'the', 'authorities', 'gorbachev', 'economy', 'reported', 'states', 'police', 'ap', 'do', 'some', 'sales', 'one', 'congress', 'docno', 'percent', 'an', 'to', 'cents', 'meeting', 'doc', 'time', 'state', 'union', 'noriega', 'trade']\n"
     ]
    }
   ],
   "source": [
    "word1 = []\n",
    "for key, value in word_list.items():\n",
    "    if value in find_index(beta_f[1]):\n",
    "        word1.append(key)\n",
    "for w in word1:\n",
    "    if w in stopwords:\n",
    "        word1.remove(w)\n",
    "print(word1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['months', 'people', 'now', 'spokesman', 'national', 'economic', 'told', 'be', 'party', 'market', 'days', 'expected', 'dukakis', 'york', 'yearold', 'man', 'no', 'prices', 'germany', 'north', 'official', 'news', 'release', 'bank', 'forces', 'war', 'being', 'percent', 'military', 'tuesday', 'day', 'its', 'doc', 'east', 'state', 'continue', 'week']\n"
     ]
    }
   ],
   "source": [
    "word2 = []\n",
    "for key, value in word_list.items():\n",
    "    if value in find_index(beta_f[2]):\n",
    "        word2.append(key)\n",
    "for w in word2:\n",
    "    if w in stopwords:\n",
    "        word2.remove(w)\n",
    "print(word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['company', 'country', 'year', 'chief', 'control', 'leader', 'city', 'long', 'officials', 'soviet', 'other', 'the', 'united', 'it', 'president', 'committee', 'reported', 'bush', 'office', 'oil', 'ap', 'as', 'docno', 'dont', 'jackson', 'percent', 'an', 'ago', 'political', 'received', 'years', 'today', 'wednesday', 'east', 'state', 'program']\n"
     ]
    }
   ],
   "source": [
    "word3 = []\n",
    "for key, value in word_list.items():\n",
    "    if value in find_index(beta_f[3]):\n",
    "        word3.append(key)\n",
    "for w in word3:\n",
    "    if w in stopwords:\n",
    "        word3.remove(w)\n",
    "print(word3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['year', 'city', 'department', 'visit', 'be', 'will', 'plan', 'about', 'united', 'expected', 'he', 'compared', 'official', 'president', 'month', 'air', 'states', 'police', 'ap', 'bank', 'in', 'war', 'docno', 'percent', 'an', 'ago', 'its', 'force', 'top', 'west', 'group', 'john']\n"
     ]
    }
   ],
   "source": [
    "word4 = []\n",
    "for key, value in word_list.items():\n",
    "    if value in find_index(beta_f[4]):\n",
    "        word4.append(key)\n",
    "for w in word4:\n",
    "    if w in stopwords:\n",
    "        word4.remove(w)\n",
    "print(word4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['country', 'government', 'year', 'people', 'spokesman', 'friday', 'offer', 'national', 'officials', 'soviet', 'had', 'monday', 'million', 'house', 'county', 'largest', 'support', 'central', 'were', 'oil', 'ap', 'fire', 'congress', 'docno', 'israel', 'an', 'military', 'is', 'today', 'time', 'state', 'trade']\n"
     ]
    }
   ],
   "source": [
    "word5 = []\n",
    "for key, value in word_list.items():\n",
    "    if value in find_index(beta_f[5]):\n",
    "        word5.append(key)\n",
    "for w in word5:\n",
    "    if w in stopwords:\n",
    "        word5.remove(w)\n",
    "print(word5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['country', 'foreign', 'people', 'than', 'american', 'economic', 'officials', 'soviet', 'his', 'party', 'including', 'public', 'house', 'minister', 'report', 'southern', 'official', 'president', 'not', 'police', 'administration', 'bill', 'docno', 'percent', 'would', 'is', 'years', 'doc', 'from', 'week', 'trade']\n"
     ]
    }
   ],
   "source": [
    "word6 = []\n",
    "for key, value in word_list.items():\n",
    "    if value in find_index(beta_f[6]):\n",
    "        word6.append(key)\n",
    "for w in word6:\n",
    "    if w in stopwords:\n",
    "        word6.remove(w)\n",
    "print(word6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['saturday', 'all', 'year', 'people', 'leader', 'thursday', 'friday', 'national', 'soviet', 'business', 'condition', 'his', 'the', 'called', 'bush', 'miles', 'states', 'police', 'money', 'ap', 'administration', 'said', 'docno', 'percent', 'an', 'tuesday', 'day', 'is', 'political', 'working', 'doc', 'wednesday', 'washington', 'group', 'week']\n"
     ]
    }
   ],
   "source": [
    "word7 = []\n",
    "for key, value in word_list.items():\n",
    "    if value in find_index(beta_f[7]):\n",
    "        word7.append(key)\n",
    "for w in word7:\n",
    "    if w in stopwords:\n",
    "        word7.remove(w)\n",
    "print(word7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['saturday', 'billion', 'and', 'telephone', 'year', 'reports', 'people', 'rose', 'rebels', 'issue', 'spokesman', 'thursday', 'center', 'head', 'national', 'told', 'officials', 'soviet', 'asked', 'chairman', 'south', 'october', 'the', 'county', 'dukakis', 'san', 'york', 'prices', 'ms', 'election', 'news', 'president', 'committee', 'reported', 'members', 'bush', 'law', 'miles', 'states', 'killed', 'police', 'groups', 'ap', 'period', 'administration', 'forces', 'bill', 'william', 'said', 'war', 'docno', 'denied', 'car', 'percent', 'times', 'night', 'military', 'tuesday', 'day', 'is', 'was', 'years', 'doc', 'member', 'wednesday', 'board', 'shamir', 'state', 'army', 'week']\n"
     ]
    }
   ],
   "source": [
    "word8 = []\n",
    "for key, value in word_list.items():\n",
    "    if value in find_index(beta_f[8]):\n",
    "        word8.append(key)\n",
    "for w in word8:\n",
    "    if w in stopwords:\n",
    "        word8.remove(w)\n",
    "print(word8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['company', 'all', 'government', 'industrial', 'and', 'year', 'foreign', 'people', 'of', 'leader', 'thursday', 'quayle', 'defense', 'national', 'american', 'economic', 'our', 'announced', 'troops', 'corp', 'soviet', 'business', 'be', 'monday', 'his', 'party', 'including', 'other', 'house', 'united', 'expected', 'authorities', 'cut', 'pay', 'church', 'north', 'trust', 'high', 'president', 'bush', 'office', 'first', 'oil', 'shot', 'children', 'woman', 'states', 'stock', 'police', 'early', 'ap', 'eastern', 'budget', 's', 'two', 'lead', 'one', 'docno', 'drug', 'percent', 'agreement', 'times', 'march', 'rate', 'countries', 'fell', 'years', 'working', 'doc', 'wednesday', 'international', 'court', 'washington', 'robert', 'army', 'union', 'left']\n"
     ]
    }
   ],
   "source": [
    "word9 = []\n",
    "for key, value in word_list.items():\n",
    "    if value in find_index(beta_f[9]):\n",
    "        word9.append(key)\n",
    "for w in word9:\n",
    "    if w in stopwords:\n",
    "        word9.remove(w)\n",
    "print(word9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
